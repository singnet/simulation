{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singularity Net Simulation\n",
    "\n",
    "This is a simulation of the Singularity Net itself.  It's purpose is to compare approaches to problems of the singularity net.  The specific study of this notebook is an initial look at the fundamental problem of how to construct AI software solutions automatically out of AI programs submitted to the Singularity Net.  The simulation tests how different solutions affect Singularity Net values such as delivering maximum utility to people seeking AI software at the lowest cost, giving everyone who makes high quality software a chance to sell, distributing credit where credit is due, and complexification to facilitate a singularity. An important value of the Singularity Net is community participation in its construction, thus we offer the simulation in the competitive arena software of OpenAI to attract the community to playing the \"Singularity\" game, in the spirit of democratic meritocracy.\n",
    "\n",
    "This simulation is implemented with a combination of Mesa agent based software and OpenAi Gym reinforcement learning competition software. Mesa handles the network of multiple agents, their rules of interaction, and their schedule, and cooperative setups, while OpenAi gym provides reinforcement learning algorithms that agents may use to learn AI combination strategies in a competitive setup.  Mesa simulation agents read and write to a network-mediated blackboard implemented as an openAi environment.  On the blackboard appears human requests for a particular algorithm type that can pass a test (that has a gradient) on a particular dataset for a particular (possibly secret) price range in AGI tokens.  The algorithm type, test, and dataset are from an ontology.   In response, agents can place on the blackboard offers to buy, sell, or construct software from other agents. Construction can include finding appropriate types from the human-designed ontology and parameter settings in combinations that fit particular use cases. However, it can also include inventing new algorithms that trade an input list for an output list (in offer net fashion) that are not listed in the human-designed ontology.   Agents may also display a sign in the form of a vector of floats, and may express their preferences for agents whose sign is closest to an ideal sign.  The sign may be used to rank agents using any agent matching algorithm.\n",
    "\n",
    "The simulation has a general, flexible knowledge representation that is meant to accomodate the needs of different reinforcement learning algorithms.  Different modular reinforcement learning algorithms may compete on the same blackboard in contests to see which algorithms best fill user, developer, and AI growth needs. Alternatively, the simulation could focus on a single algorithm, measuring its effects in isolation, in a more cooperative scenario.  The different algorithms implement agent integration schemes with their own approaches to satisfying the needs of users,devlopers, and growing artificial intelligence. For example, it is up to each reinforcement learning algorithm to assign meanings to the float vector signs that they read and display on the blackboard. Different possible interpretations of the sign's float dimensions include reputation scores, the identity of an agent, the information needed for offernet trades, an emergent agent language or an emergent software ontology that augments the human designed ontology.  Agents may also require that software pass certain tests on data, seen and unseen, before a transaction is accepted.  They have the option to advertise that their software has already passed tests on data.  They have the choice to indicate the type of software they are buying, selling, or constructing with the human ontology, which indicates an input and output list for them.  Alternatively they may offer an input list for an output list and allow the sign field to represent the ontological type.  They have the choice of paying solely through the input and output list without using agi tokens for currency, in order to implement an offer net type of scenario.  During competition, different meanings of signs could either isolate agents into solution groups having the same algorithm, or algorithms groups could learn other group's meanings, creating a competition on the meaning of signs.\n",
    "\n",
    "Once all offers are on the blackboard, the openAI environment makes the matches ranked by the cosine distance of the signs that have overlapping prices ranges and exceed agents stated test threshold criteria.  The environment creates the software, and distributes funds.  It funds through the openAi environment reward signal, detailing the particular float vectors that were rewarded through the observation signal.  However it is up to the particular reinforcement learning algorithm to choose what aspects of the environment reward, observation,  and of itself to include in its personal reward.     Mesa keeps track of agent and human utility satisfaction and prices, and measures singularity net values such as adequate exploration of unknown agent capabilities.  \n",
    "\n",
    "This general design promotes Singularity Net values in several ways.  This simulation allows credit to be assigned through the market price signal. However, agents can also gain through more offer-net type utility based trades, for example, use of the input list for training, or more reputation utility based trade, for example, volunteering to increase ones reputation, to ultimately increase its market value in AGI tokens. Correct assignment of credit is needed to promote quality software that satisfys the user, while at the same time assures that developers have a fair shot.  It is expected that agents that learn to  do jobs of uniquely higher quality in uniquely necessary areas will be able to charge higher prices.  Since agents are stateful and able to gain experience from a variety of different problems, it is expected that one way for an agent to gain a higher price would be to reuse and accumulate knowledge in one area, that is, to specialize.  Such agents would contain parameters and functions suited to particular types of problems.  Importantly, these problems need not be expressed in the human designed ontology:  they may involve whatever agent roles are needed in practice to get the job done.  The emergence of automated new constructions and new ontological categories is prerequisite to AIs which can construct themselves and thus supports a singularity.   As agents learn how to buy and sell the services of other agents, particularly higher quality but cheaper agents that they have learned to recognize through sign and test, it is expected that curating agents will emerge.  Curation agents that can both recommend software sucessfully and give new developers a chance to become known satisfy both user and developer.  \n",
    "\n",
    "First, in part one, we introduce the knowledge representation of the communicating agents on the blackboard.  In part two, we demonstrate an initial use of the simulation with a particular reinforcement learning algorithm to construct the AI software, SISTER (Symbolic Interactionist Simulation of Trade and Emergent Roles). SISTER is an evolutionary algorithm that implements Agent-based Coevolution.  This algorithm uses the sign areas of the blackboard communications to learn specialized, non-preprogrammed roles for the agents, emerging needed to solve problems.  Utility/fitness/objective function for individual agents is measured in AGI tokens, making use of Adam Smith's invisible hand rather than group selection coevolution methods that base utility on the good of the whole.  Assignment of credit through the market process is inherit in this method.   As her name suggests, emergent roles in SISTER are macro institutions from the micro-macro integration processes of micro-economics and micro-sociological symbolic interactionism.  Thus SISTER models growth in AI on growth in economy and society."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.  Representation for Evolvability\n",
    "  \n",
    "\n",
    "This simulation uses a \"pickled curry\" representation of python that addresses problems of wastefulness in combinatorial search techniques such as evolutionary algorithms, while at the same time making it easier to evolve AI solutions composed of existing python programs.  Since python is not a tree based language, it is more difficult to evolve than are tree based, functional languages such as Scala, especially those that can easily use their program representations as data such as Scheme.  However, since it is the most used language in data science, we want to be able to put together lego blocks of existing python programs. Python has a few functional programming representations, such as lamdas for unnamed functions.  It does not have currying, but we found an implemenation of currying in python to use.  The pickling of bound curries in this preliminary implementation does not make use of introspection or marshalling, which would be important in making curries that were mobile across machines.  Since pickles can be malicious it is recommended that security issues be handled before this prototype simulation is actually used in competitions.  However, when used for a simulation on a single machine, these curried pickles are saved to disk and kept track of to ensure that nothing need be computed more than once. The code that corresponds to the chosen pickles may be saved separately, using more sophistication than is used in this prototype. However, once security issues are resolved, the curry representation in itself is good for dividing a problem up into parts on mulitple machines for high performance computing, and serialization in itself is good for checkpoint storage of computationally intense functions as are many machine learning functions.  \n",
    "\n",
    "Our curry representation constrains python to evolvable pipes, incrementally binding parameters in a seqential manner. Curried programs are closely tied to a tree based representation.  However there is one difference between our curries and many tree based representations that machine learning algorithms such as evolutionary programming are applied to.  Tree based genetic programming has difficulty converging in ways that dont grow too quickly, and in agents that base their interactions on the equilibria of economics, convergence represents a solution, a dynamic compromise, and an institution. SISTER, for example, depends upon the compromise of convergence between agents to develop the institutions of role and role relations that are the solutions to AI integration problems. Trees offer the ability for different modifiers to refer to the same entity without having to rename it, a great advantage in genetic programming (GP).  However, GP programs have difficulty in converging, because growing trees often involve displacing insertions, with small changes in genotype resulting in large changes in phenotype. That is why we combine currying with agents, so that we take advantage of referring to the same entity as trees do, but multiple branches of a tree (forks) are implmented with multiple agents.  In our curried representation, a position corresponds to itself in the next generation so that convergence may occur. However, chromosome size can change as is needed for growth not by insertion but using markers, that is , a stop codon (marker) that moves through introns that lengthens a chromosome but preserves position.  The hierarchical tree structure of the curries also gives gradient to machine learning programs by defining what is a close and what is a distant change, becoming a kind of a gray coding that makes changes that are close in phenotype, close in genotype and changes that are far in phenotype, far in genotype.    \n",
    "\n",
    "The knowledge representation of the agent communications is a vector of floats sent to the openAi blackboard environment as an action. A floating point representation was chosen because it is one of the most general, that can be used or converted into use by any machine learning algorithm.  It makes search in the float parameter space quite easy to add on at a later point. It uses the curry representation, and likewise is designed for evolvability by faciliating convergence and gradient, needed by many machine learning algorithms.  For example, the SISTER algorithm uses the float vector of agent communications as the chromosome inside of an individual CMA-ES algorithm within each agent.  \n",
    "\n",
    "The blackboard knowledge representation for agent communications, a vector of floats from zero to one is as follows:\n",
    "\n",
    "sign_to_display \\[float\\*sign_size\\]\n",
    "\n",
    "num_blocks \\*\\(\n",
    "\n",
    "type\\[1 float: construct, buy , sell, or stop\\] \n",
    "\n",
    "item \\[float\\*item_size: interpretaton comes from ontology\\] (Uses stop codons)\n",
    "\n",
    "input item \\[float\\*item_size: interpretaton comes from ontology\\] (Uses stop codons) (not implemented in example)\n",
    "\n",
    "output item \\[float\\*item_size: interpretaton comes from ontology\\] (Uses stop codons)(not implemented in example)\n",
    "\n",
    "test \\[num_tests\\* (float\\*item_size for the test program, float\\* item_size for the data to test, 1 float for threshold,  1 float for boolean is_hidden)\\] (Uses stop codons) \n",
    "\n",
    "price_range (float\\*2: interpreted with min_amount and max_amount)\n",
    "\n",
    "sign_to_seek(float\\*sign_size)\n",
    "\n",
    "\\)\n",
    "\n",
    "The simulation config parameters listed above are in the curriedPickles.json json config file.  As an example, if the parmeters are set to these in the config file:\n",
    "\n",
    "\t\t\"sign_size\":8, \n",
    "\t\t\"num_blocks\":10,\n",
    "\t\t\"item_size\": 8, \n",
    "\t\t\"num_tests\":5,\n",
    "\t\t\"min_token_price\":1, \n",
    "\t\t\"max_token_price\":100\n",
    "        \n",
    "Then each message of floats from an agent to the blackboard is a vector of (8+10*(1+8+8+8+5*(8+8+1+1)+2+8) = 1258 floats.\n",
    "\n",
    "The sign to display on the blackboard is a 8 float long vector, which is compared to the (sign to seek) field of someone seeking out the agent for an offer.  The cosine distance ranks agents according to how closely they resemble the ideal agent.  It is a general computation of distance that can be used in a variety of algorithms.  \n",
    "\n",
    "Each block represents a different trade offer or construction notice to put on the blackboard. An agent can communicate up to num_blocks blocks, but the number of actual communications is controlled by the stop codon.\n",
    "\n",
    "An agent may indicate an item to sell, buy or construct, as well as an input list and an output list for that item.  An agent can sell what they have constructed or have bought, however the desire to sell constructed software is implied in its posting on the blackboard with a price.  \n",
    "\n",
    "The human-designed ontology is an api of apis that includes an input list and output list in \\*args, \\*\\*kwargs python syntax.  It is used if stop codons appear in the input and output fields, which would be in cases without emergent ontological categories (most cases).  The input and output lists are for cases when the software bought sold or constructed does not occur in the human-designed api, because the agents have invented new software in new categores indicated in the sign fields.  For example, the agents could conceivably invent a new clusterer that wasnt one of the listed ones, in which case they they may indicate a clusterer using the human-designed general category with a stop codon immediately following, and then specify an input and output list.  Because the input and output list is used with software references, it only needs the \\*args syntax.  An offer network example would include only a stop codon in the item field, zero in the price field, and the input and output list, with the sign field taking care of the ontology.  Input and output data structures are not included in the ontology of this example.\n",
    "\n",
    "The tests are optional to the agent and used in a buy block to require that software pass with a threshold before it is accepted, with or without revealing the test and data to pass (so that it cant be gamed or trained on).  The same tests in the construct and the sell block indicate that the agent has passed the test, before the item was put on the board, and to have this hidden is a note to the agents self to pass the test before it is put on the board.  Agents do not need such tests in that they could rely on reputation or market price alone to incentivize themselves to quality, but such tests are good points of quality control, and necessary when dealing directly with human beings in establishing criteria for transaction validation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The API of APIs\n",
    "\n",
    "The human-designed ontology is a particular instance of an API of APIs, in a JSON file.  The official Singularity Net ontology or API of APIs would contain all of the software entered into the singularity net, but this smaller curried pickle representation of the API of APIs is used solely for evolution, and read the official ontology. Different instances of the ontology used in evolution should be created for individual problems, so that the functions that are made available to use in a solution can be reduced to those that are likely to be in that solution over a threshold, so that those functions may be weighted by their likelihood to be in a solution, and so that different parameter values to explore together may be indicated. All these statistical relations between functions and particular solutions can be learned from current solutions in the open source, as in the Microsoft DeepCoder project (https://openreview.net/pdf?id=ByldLrqlx).  However, the hierarchical design of this evolvable ontology could also feed the official version, as the foundation for a convenient checklist that humans could use to specify their desired services, that would take little translation into a version used for evolution. GUIs with checklists could be used by both users and developers to generate the representation.    \n",
    "\n",
    "The goal of this study is not to define an api of apis with software pattern commonalities so that all of the apis can work together, the central task of the design of an API of APIs. Rather, our goal is to look at general structures to express these commonalities in evolvable and scalable ways that are useful to both integrating agents and humans.  Specifically, we leave the details of data structures, formats, and IO out of the ontology, although we assume that they can be expressed in the same hierarchical format that software functions are. Common and useful data formats as hubs as well as their spoke translators are needed to connect disparate functions together in any manner, including in the pipes and forks of curries of this example.  Here we use the popular format, the pandas Dataframe.  For this prototype, the input and output of functions is the Dataframe with standardized column headings.  We leave data transfer studies for the future so that we can now concentrate on python function evolvability, scalability for evolvability, and a way to keep track of and reuse what has been computed before. \n",
    "\n",
    "\n",
    "Incidentally, in the real version of the API of APIs,  all the software of the SingularityNET will be represented, including the simulation of the singularity net itself.  If the simulation were to incorporate the real API, then the simulation could be included in the simulation.  Including the simulation in the simulation, with the checkpoint and restart of the curried pickles to inspect different branches that could be taken, is important to strategic modelling, for example the coevolutionary recursion of models of attacks upon and defense of the singularity net.   \n",
    "\n",
    "Here is the ontology, the API of APIs, for this simulation example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"test\": {\n",
      "    \"_weight\": 0.3,\n",
      "    \"clusterer\": {\n",
      "      \"Silhouette\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"test_clusterer_silhouette.p\",\n",
      "        \"_kwargs\": {\n",
      "          \"time\": {\n",
      "            \"type\": \"float32\"\n",
      "          }\n",
      "        },\n",
      "        \"_args\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"float32\"\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"int32\"\n",
      "          }\n",
      "        ],\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"float\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"_weight\": 0.3,\n",
      "    \"freetext\": {\n",
      "      \"internetResearchAgency\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"data_freetext_internetResearchAgency.p\",\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"pandas.core.frame.DataFrame\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"BSdetector\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"data_freetext_BSdetector.p\",\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"pandas.core.frame.DataFrame\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"short\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"data_freetext_short.p\",\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"pandas.core.frame.DataFrame\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    \"vector\": {\n",
      "      \"_weight\": 0.3,\n",
      "      \"blobs\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"data_vector_blobs.p\",\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"pandas.core.frame.DataFrame\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"preprocessor\": {\n",
      "    \"_weight\": 0.3,\n",
      "    \"freetext\": {\n",
      "      \"_weight\": 0.3,\n",
      "      \"emojiRemoval\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"data_freetext_internetResearchAgency.p\",\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"pandas.core.frame.DataFrame\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"lemmatization\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"data_freetext_BSdetector.p\",\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"pandas.core.frame.DataFrame\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"stopwords\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"data_vector_blobs.p\",\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"pandas.core.frame.DataFrame\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"clusterer\": {\n",
      "    \"_weight\": 0.3,\n",
      "    \"sklearn\": {\n",
      "      \"_weight\": 0.3,\n",
      "      \"kmeans\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"clusterer_sklearn_kmeans.p\",\n",
      "        \"_kwargs\": {\n",
      "          \"n_clusters\": {\n",
      "            \"type\": \"int\"\n",
      "          }\n",
      "        },\n",
      "        \"_args\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"float32\"\n",
      "          }\n",
      "        ],\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"int32\"\n",
      "          }\n",
      "        ],\n",
      "        \"5clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_kmeans_5clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 5\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"10clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_kmeans_10clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 10\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"20clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_kmeans_20clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 20\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      \"agglomerative\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"clusterer_sklearn_agglomerative.p\",\n",
      "        \"_kwargs\": {\n",
      "          \"n_clusters\": {\n",
      "            \"type\": \"int\"\n",
      "          }\n",
      "        },\n",
      "        \"_args\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"float32\"\n",
      "          }\n",
      "        ],\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"int32\"\n",
      "          }\n",
      "        ],\n",
      "        \"5clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_agglomerative_5clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 5\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"10clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_agglomerative_10clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 10\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"20clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_agglomerative_20clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 20\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      \"affinityPropagation\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"clusterer_sklearn_affinityPropagation.p\",\n",
      "        \"_kwargs\": {\n",
      "          \"n_clusters\": {\n",
      "            \"type\": \"int\"\n",
      "          }\n",
      "        },\n",
      "        \"_args\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"float32\"\n",
      "          }\n",
      "        ],\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"int32\"\n",
      "          }\n",
      "        ],\n",
      "        \"5clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_affinityPropagation_5clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 5\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"10clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_affinityPropagation_10clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 10\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"20clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_affinityPropagation_20clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 20\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      \"meanShift\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"clusterer_sklearn_meanShift.p\",\n",
      "        \"_kwargs\": {\n",
      "          \"n_clusters\": {\n",
      "            \"type\": \"int\"\n",
      "          }\n",
      "        },\n",
      "        \"_args\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"float32\"\n",
      "          }\n",
      "        ],\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"int32\"\n",
      "          }\n",
      "        ],\n",
      "        \"5clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_meanShift_5clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 5\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"10clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_meanShift_10clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 10\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"20clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_meanShift_20clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 20\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      \"spectral\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"clusterer_sklearn_spectral.p\",\n",
      "        \"_kwargs\": {\n",
      "          \"n_clusters\": {\n",
      "            \"type\": \"int\"\n",
      "          }\n",
      "        },\n",
      "        \"_args\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"float32\"\n",
      "          }\n",
      "        ],\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"int32\"\n",
      "          }\n",
      "        ],\n",
      "        \"5clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_spectral_5clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 5\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"10clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_spectral_10clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 10\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"20clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_spectral_20clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 20\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      \"ward\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"clusterer_sklearn_ward.p\",\n",
      "        \"_kwargs\": {\n",
      "          \"n_clusters\": {\n",
      "            \"type\": \"int\"\n",
      "          }\n",
      "        },\n",
      "        \"_args\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"float32\"\n",
      "          }\n",
      "        ],\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"int32\"\n",
      "          }\n",
      "        ],\n",
      "        \"5clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_ward_5clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 5\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"10clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_ward_10clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 10\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"20clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_ward_20clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 20\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      \"dbscan\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"clusterer_sklearn_dbscan.p\",\n",
      "        \"_kwargs\": {\n",
      "          \"n_clusters\": {\n",
      "            \"type\": \"int\"\n",
      "          }\n",
      "        },\n",
      "        \"_args\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"float32\"\n",
      "          }\n",
      "        ],\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"int32\"\n",
      "          }\n",
      "        ],\n",
      "        \"5clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_dbscan_5clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 5\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"10clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_dbscan_10clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 10\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"20clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_dbscan_20clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 20\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      \"birch\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"clusterer_sklearn_birch.p\",\n",
      "        \"_kwargs\": {\n",
      "          \"n_clusters\": {\n",
      "            \"type\": \"int\"\n",
      "          }\n",
      "        },\n",
      "        \"_args\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"float32\"\n",
      "          }\n",
      "        ],\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"int32\"\n",
      "          }\n",
      "        ],\n",
      "        \"5clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_birch_5clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 5\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"10clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_birch_10clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 10\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"20clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_birch_20clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 20\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      \"gaussian\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"clusterer_sklearn_gaussian.p\",\n",
      "        \"_kwargs\": {\n",
      "          \"n_clusters\": {\n",
      "            \"type\": \"int\"\n",
      "          }\n",
      "        },\n",
      "        \"_args\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"float32\"\n",
      "          }\n",
      "        ],\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"int32\"\n",
      "          }\n",
      "        ],\n",
      "        \"5clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_gaussian_5clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 5\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"10clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_gaussian_10clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 10\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"20clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_sklearn_gaussian_20clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 20\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    \"nltk\": {\n",
      "      \"kmeans\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"clusterer_nltk_kmeans.p\",\n",
      "        \"_kwargs\": {\n",
      "          \"n_clusters\": {\n",
      "            \"type\": \"int\"\n",
      "          }\n",
      "        },\n",
      "        \"_args\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"float32\"\n",
      "          }\n",
      "        ],\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"int32\"\n",
      "          }\n",
      "        ],\n",
      "        \"5clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_nltk_kmeans_5clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 5\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"10clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_nltk_kmeans_10clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 10\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"20clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_nltk_kmeans_20clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 20\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      \"agglomerative\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"clusterer_nltk_agglomerative.p\",\n",
      "        \"_kwargs\": {\n",
      "          \"n_clusters\": {\n",
      "            \"type\": \"int\"\n",
      "          }\n",
      "        },\n",
      "        \"_args\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"float32\"\n",
      "          }\n",
      "        ],\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"int32\"\n",
      "          }\n",
      "        ],\n",
      "        \"5clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_nltk_agglomerative_5clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 5\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"10clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_nltk_agglomerative_10clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 10\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        },\n",
      "        \"20clusters\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"clusterer_nltk_agglomerative_20clusters.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"n_clusters\": 20\n",
      "          },\n",
      "          \"_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"vectorSpace\": {\n",
      "    \"gensim\": {\n",
      "      \"doc2vec\": {\n",
      "        \"_weight\": 0.3,\n",
      "        \"_pickle\": \"vectorSpace_gensim_doc2vec.p\",\n",
      "        \"_kwargs\": {\n",
      "          \"size\": {\n",
      "            \"type\": \"int\"\n",
      "          },\n",
      "          \"iterations\": {\n",
      "            \"type\": \"int\"\n",
      "          },\n",
      "          \"minfreq\": {\n",
      "            \"type\": \"int\"\n",
      "          }\n",
      "        },\n",
      "        \"_args\": [\n",
      "          {\n",
      "            \"type\": \"pandas.core.frame.DataFrame\"\n",
      "          }\n",
      "        ],\n",
      "        \"_return\": [\n",
      "          {\n",
      "            \"type\": \"numpy.ndarray\",\n",
      "            \"dtype\": \"float32\"\n",
      "          }\n",
      "        ],\n",
      "        \"50size\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"vectorSpace_gensim_doc2vec_50size.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"size\": 50\n",
      "          },\n",
      "          \"_kwargs\": {\n",
      "            \"iterations\": {\n",
      "              \"type\": \"int\"\n",
      "            },\n",
      "            \"minfreq\": {\n",
      "              \"type\": \"int\"\n",
      "            }\n",
      "          },\n",
      "          \"20iterations\": {\n",
      "            \"_weight\": 0.3,\n",
      "            \"_pickle\": \"vectorSpace_gensim_doc2vec_50size_20iterations.p\",\n",
      "            \"_kwarg_vals\": {\n",
      "              \"iterations\": 20\n",
      "            },\n",
      "            \"_kwargs\": {\n",
      "              \"minfreq\": {\n",
      "                \"type\": \"int\"\n",
      "              }\n",
      "            },\n",
      "            \"2minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_50size_20iterations_2minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 2\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            },\n",
      "            \"5minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_50size_20iterations_5minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 5\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            }\n",
      "          },\n",
      "          \"200iterations\": {\n",
      "            \"_weight\": 0.3,\n",
      "            \"_pickle\": \"vectorSpace_gensim_doc2vec_50size_200iterations.p\",\n",
      "            \"_kwarg_vals\": {\n",
      "              \"iterations\": 200\n",
      "            },\n",
      "            \"_kwargs\": {\n",
      "              \"minfreq\": {\n",
      "                \"type\": \"int\"\n",
      "              }\n",
      "            },\n",
      "            \"2minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_50size_200iterations_2minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 2\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            },\n",
      "            \"5minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_50size_200iterations_5minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 5\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            }\n",
      "          },\n",
      "          \"1000iterations\": {\n",
      "            \"_weight\": 0.3,\n",
      "            \"_pickle\": \"vectorSpace_gensim_doc2vec_50size_1000iterations.p\",\n",
      "            \"_kwarg_vals\": {\n",
      "              \"iterations\": 1000\n",
      "            },\n",
      "            \"_kwargs\": {\n",
      "              \"minfreq\": {\n",
      "                \"type\": \"int\"\n",
      "              }\n",
      "            },\n",
      "            \"2minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_50size_1000iterations_2minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 2\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            },\n",
      "            \"5minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_50size_1000iterations_5minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 5\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"100size\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"vectorSpace_gensim_doc2vec_100size.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"size\": 100\n",
      "          },\n",
      "          \"_kwargs\": {\n",
      "            \"iterations\": {\n",
      "              \"type\": \"int\"\n",
      "            },\n",
      "            \"minfreq\": {\n",
      "              \"type\": \"int\"\n",
      "            }\n",
      "          },\n",
      "          \"20iterations\": {\n",
      "            \"_weight\": 0.3,\n",
      "            \"_pickle\": \"vectorSpace_gensim_doc2vec_100size_20iterations.p\",\n",
      "            \"_kwarg_vals\": {\n",
      "              \"iterations\": 20\n",
      "            },\n",
      "            \"_kwargs\": {\n",
      "              \"minfreq\": {\n",
      "                \"type\": \"int\"\n",
      "              }\n",
      "            },\n",
      "            \"2minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_100size_20iterations_2minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 2\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            },\n",
      "            \"5minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_100size_20iterations_5minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 5\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            }\n",
      "          },\n",
      "          \"200iterations\": {\n",
      "            \"_weight\": 0.3,\n",
      "            \"_pickle\": \"vectorSpace_gensim_doc2vec_100size_200iterations.p\",\n",
      "            \"_kwarg_vals\": {\n",
      "              \"iterations\": 200\n",
      "            },\n",
      "            \"_kwargs\": {\n",
      "              \"minfreq\": {\n",
      "                \"type\": \"int\"\n",
      "              }\n",
      "            },\n",
      "            \"2minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_100size_200iterations_2minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 2\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            },\n",
      "            \"5minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_100size_200iterations_5minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 5\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            }\n",
      "          },\n",
      "          \"1000iterations\": {\n",
      "            \"_weight\": 0.3,\n",
      "            \"_pickle\": \"vectorSpace_gensim_doc2vec_100size_1000iterations.p\",\n",
      "            \"_kwarg_vals\": {\n",
      "              \"iterations\": 1000\n",
      "            },\n",
      "            \"_kwargs\": {\n",
      "              \"minfreq\": {\n",
      "                \"type\": \"int\"\n",
      "              }\n",
      "            },\n",
      "            \"2minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_100size_1000iterations_2minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 2\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            },\n",
      "            \"5minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_100size_1000iterations_5minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 5\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"200size\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"vectorSpace_gensim_doc2vec_200size.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"size\": 200\n",
      "          },\n",
      "          \"_kwargs\": {\n",
      "            \"iterations\": {\n",
      "              \"type\": \"int\"\n",
      "            },\n",
      "            \"minfreq\": {\n",
      "              \"type\": \"int\"\n",
      "            }\n",
      "          },\n",
      "          \"20iterations\": {\n",
      "            \"_weight\": 0.3,\n",
      "            \"_pickle\": \"vectorSpace_gensim_doc2vec_200size_20iterations.p\",\n",
      "            \"_kwarg_vals\": {\n",
      "              \"iterations\": 20\n",
      "            },\n",
      "            \"_kwargs\": {\n",
      "              \"minfreq\": {\n",
      "                \"type\": \"int\"\n",
      "              }\n",
      "            },\n",
      "            \"2minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_200size_20iterations_2minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 2\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            },\n",
      "            \"5minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_200size_20iterations_5minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 5\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            }\n",
      "          },\n",
      "          \"200iterations\": {\n",
      "            \"_weight\": 0.3,\n",
      "            \"_pickle\": \"vectorSpace_gensim_doc2vec_200size_200iterations.p\",\n",
      "            \"_kwarg_vals\": {\n",
      "              \"iterations\": 200\n",
      "            },\n",
      "            \"_kwargs\": {\n",
      "              \"minfreq\": {\n",
      "                \"type\": \"int\"\n",
      "              }\n",
      "            },\n",
      "            \"2minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_200size_200iterations_2minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 2\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            },\n",
      "            \"5minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_200size_200iterations_5minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 5\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            }\n",
      "          },\n",
      "          \"1000iterations\": {\n",
      "            \"_weight\": 0.3,\n",
      "            \"_pickle\": \"vectorSpace_gensim_doc2vec_200size_1000iterations.p\",\n",
      "            \"_kwarg_vals\": {\n",
      "              \"iterations\": 1000\n",
      "            },\n",
      "            \"_kwargs\": {\n",
      "              \"minfreq\": {\n",
      "                \"type\": \"int\"\n",
      "              }\n",
      "            },\n",
      "            \"2minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_200size_1000iterations_2minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 2\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            },\n",
      "            \"5minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_200size_1000iterations_5minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 5\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"300size\": {\n",
      "          \"_weight\": 0.3,\n",
      "          \"_pickle\": \"vectorSpace_gensim_doc2vec_300size.p\",\n",
      "          \"_kwarg_vals\": {\n",
      "            \"size\": 300\n",
      "          },\n",
      "          \"_kwargs\": {\n",
      "            \"iterations\": {\n",
      "              \"type\": \"int\"\n",
      "            },\n",
      "            \"minfreq\": {\n",
      "              \"type\": \"int\"\n",
      "            }\n",
      "          },\n",
      "          \"20iterations\": {\n",
      "            \"_weight\": 0.3,\n",
      "            \"_pickle\": \"vectorSpace_gensim_doc2vec_300size_20iterations.p\",\n",
      "            \"_kwarg_vals\": {\n",
      "              \"iterations\": 20\n",
      "            },\n",
      "            \"_kwargs\": {\n",
      "              \"minfreq\": {\n",
      "                \"type\": \"int\"\n",
      "              }\n",
      "            },\n",
      "            \"2minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_300size_20iterations_2minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 2\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            },\n",
      "            \"5minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_300size_20iterations_5minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 5\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            }\n",
      "          },\n",
      "          \"200iterations\": {\n",
      "            \"_weight\": 0.3,\n",
      "            \"_pickle\": \"vectorSpace_gensim_doc2vec_300size_200iterations.p\",\n",
      "            \"_kwarg_vals\": {\n",
      "              \"iterations\": 200\n",
      "            },\n",
      "            \"_kwargs\": {\n",
      "              \"minfreq\": {\n",
      "                \"type\": \"int\"\n",
      "              }\n",
      "            },\n",
      "            \"2minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_300size_200iterations_2minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 2\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            },\n",
      "            \"5minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_300size_200iterations_5minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 5\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            }\n",
      "          },\n",
      "          \"1000iterations\": {\n",
      "            \"_weight\": 0.3,\n",
      "            \"_pickle\": \"vectorSpace_gensim_doc2vec_300size_1000iterations.p\",\n",
      "            \"_kwarg_vals\": {\n",
      "              \"iterations\": 1000\n",
      "            },\n",
      "            \"_kwargs\": {\n",
      "              \"minfreq\": {\n",
      "                \"type\": \"int\"\n",
      "              }\n",
      "            },\n",
      "            \"2minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_300size_1000iterations_2minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 2\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            },\n",
      "            \"5minFreq\": {\n",
      "              \"_weight\": 0.3,\n",
      "              \"_pickle\": \"vectorSpace_gensim_doc2vec_300size_1000iterations_5minFreq.p\",\n",
      "              \"_kwarg_vals\": {\n",
      "                \"minFreq\": 5\n",
      "              },\n",
      "              \"_kwargs\": {}\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('curriedPickles.json') as json_file:  \n",
    "    config = json.load(json_file)\n",
    "    print(json.dumps(config['ontology'], indent=2))\n",
    "   # for type in config['ontology']:\n",
    "        #print('Name: ' + type['name'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hierarchical representation in the ontology (api of apis) has consistent levels, from root to branch, of function type, datatype or brand, algorithm, parameter1,parmeter 2 ,parameter3, etc. As consistancy is important for evolvability, care is taken in creating the JSON file to list parmeters in a consistent order, for example the number of clusters in clustering algorithms might all be listed first. The siblings within a level are also listed in a meaningful order, for example all clustering algorithms might should be sampled at 10 clusters, 30 clusters and 50 clusters.  Some paths from root to branch in the example ontology are test-clusterer-silouhette, data-freetext-internetResearchAgency, and vectorSpace-gensim-doc2vec-50size-200iterations-2minFrq.   50size, 200iterations, and 2minFrq are three parameter values that our ontology designates should be tested together in gensim's doc2vec algorithm, a vectorSpace algorithm. Admitedly there are many more possible combinations of parameters that can be explored than can be explicated individually in a JSON file, but these are the ones that are worth saving to disk as checkpoints. Algorithms can still be used to zero in on exact float values and parmeter relations, but we would not save all combinations of these parmeter values to disk.   Because we are dealing with curried functions, a function with one of the parmeters bound is itself a function.  From root to branch we go from more general to more and more specific functions, until when all values are bound, the output of the function only differs if it is stochastic.  So as we go from left to right , we go from broader to narrower posiblities.  \n",
    "\n",
    "Internal variables as opposed to the next level of the tree start with an underscore, and inherit values from parent nodes if they are not specified in a child node.  The internal variables at each level include those we would expect in an api, the types of the input and output, in key word arguments as well as arguments, plus a pickle file and a weight.  The ontology doesnt say that each of the pickles in the curried functions to the right of the basic function exist, only that if they are pickled and saved, what their name would be.  Currently, only completely bound and called functions are pickled, but there may be times when the computation/space tradeoff or the division of computation across machines calls for the pickling (or marshalling) of partially bound functions. In the ontology file, the input values that are not bound are the ones expected from other agents. If two or more inputs are not bound, then the input of two or more agents is required. Although more than one output is allowed in python, they are not in this protptype curried representation. The weight internal variable is normalized with its siblings, and represents the probability that that a sibling node occurs in a solution given the parent node. It can be filled in with data from techniques that use the likelihood of function use given the problem such as Microsoft's DeepCoder. Right now, however, the liklihood of any child is the same.  \n",
    "\n",
    "The hierarchical design of functions gives gradient to the vector of floats representation of an item (to buy, sell, or construct)  in the agent communication to the blackboard. Each float in the float vector representation of an item represents a level in the hierarchy, with the values of floats to the left determining the meanings of floats to the right.  The first float in this hierarchy, is whether the node is a test, or data, or preprocessor, or vectorSpace or clusterer.  Since in this case, they are all equally weighted, each has a 20% chance of being picked. If, say, the float in position 1 fell in the range of clusterer, of values .8 and above, then floats above .5 in position 2 might indicate the brand NLTK, since there are two brands.  Since the string ends with a stop codon on the left, and more specific answers occur on the left, a consensus about general facts about the item may form, or converge, before the specific facts about an item.  This representation works with gene switches which can be disruptive, but the consistency of the meanings of the levels helps to mitigate disruptions to the left.   For example, a 300 size vector would mean the same thing in a vector space whether the brand was NLTK or scikit learn.  \n",
    "\n",
    "Now we are in a position to interpret agent communication on the board.  First we look at a scenario of individual float communications on the blackboard, interpreting each float and how they construct a program. Next we take the steps to construct a program from those same floats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Interpretation of the vector of floats that agents communicate with on the blackboard\n",
    "\n",
    "\n",
    "The blackboard is intialized with five offers made by three humans.  One of the humans buys a test and data from other humans, and asks singularity net to construct some software, a clusterer of the Internet Research Agency tweets, and to add a column with cluster distance to a dataframe.\n",
    "\n",
    "In interpreting the vector of floats on the blackboard remember that, except for the signs, each float from 0 - 1 is divided up by the probability of a value.  0.37 is interpreted as a sell because the possible values (alleles), buy, sell, construct, and stop are evenly distributed, and 0.37 is in the sell range from 25 to 49.  In the ontology, the allele probabilies are weighted and normalized.\n",
    "\n",
    "___\n",
    "_____\n",
    "\n",
    "0000000 .4 sign reserved for human\n",
    "\n",
    "0.37 sell\n",
    "\n",
    "0.22 data  0.4 freetext 0.17 internetResearchAgency .99 stop ... points to data in the api of apis\n",
    "\n",
    ".87...  test type not chosen \n",
    "\n",
    "0.11 0.27 accepts between 11 and 27 agiTokens (hidden)\n",
    "\n",
    "00000000 sign not used by human\n",
    "\n",
    "_____\n",
    "\n",
    "\n",
    "\n",
    "0000000 .3 sign reserved for human\n",
    "\n",
    "0.58 sell\n",
    "\n",
    "0.18 test  0.4 clusterer 0.88 silhouette .99 stop ...  points to test in api of apis\n",
    "\n",
    ".59...  test type not chosen \n",
    "\n",
    "0.34 0.45 accepts between 34 and 45 agiTokens (hidden)\n",
    "\n",
    "00000000 sign not used by human\n",
    "\n",
    "_____\n",
    "\n",
    "\n",
    "0000000 .2 sign reserved for human\n",
    "\n",
    "0.18 buy\n",
    "\n",
    "0.22 data  0.4 freetext 0.17 internetResearchAgency .99 stop ...  points to data in the api of apis\n",
    "\n",
    ".24... test type not chosen \n",
    "\n",
    "0.27  0.76 accepts between 25 and 76 agiTokens  \n",
    "\n",
    "00000000 sign not used by human\n",
    "\n",
    "0.08 buy\n",
    "\n",
    "0.18 test  0.4 clusterer 0.88 silhouette  .99 stop ....  points to test in api of apis\n",
    "\n",
    ".78 ...test type not chosen \n",
    "\n",
    "0.34 0.41 accepts between 34 and 41 agiTokens \n",
    "\n",
    "00000000 sign not used by human\n",
    "\n",
    "0.08 buy\n",
    "\n",
    "0.65 clusterer .99 stop ...\n",
    "\n",
    "(the cluster it buys must take freetext in a dataframe in the column named text and output a dataframe of numbers in the field named cluster)\n",
    "\n",
    ".18 test  0.4 clusterer 0.88 silhouette .99 stop ....  0.22 data  0.4 freetext 0.17 internetResearchAgency  . .99 stop ....\n",
    "0.54 threshold .67 hidden .93 stop\n",
    "\n",
    "0.34 0.41 accepts between 24 and 34 agiTokens \n",
    "\n",
    "00000000 sign not used by human\n",
    "\n",
    "_____\n",
    "_____\n",
    "\n",
    "Two constructing agents are minimal for this problem because of the two data streams coming into the silouhette test.  Here is the communication of the first automated agent:\n",
    "\n",
    "_____\n",
    "\n",
    ".68 .2 .34 .52 .31 .95 .28 .46 sign displayed\n",
    "\n",
    "0.08 construct\n",
    "\n",
    "0.7 clusterer  0.43 sklearn 0.13 kmeans .80 20clusters .32 ... (stop not necesary if there are no parameters left)\n",
    "\n",
    ".28 ... test type not chosen \n",
    "\n",
    "0.1 0.4 accepts between 10 and 40 agiTokens \n",
    "\n",
    ".27 .85 .03 .24 .95 .12 .37 .75 sign sought\n",
    "\n",
    "0.21 buy vectorSpace .99stop ...\n",
    "\n",
    ".76 ...test type not chosen \n",
    "\n",
    "0.34 0.41 accepts between 34 and 41 agiTokens \n",
    "\n",
    ".33 75 .94 .476 .06 .26 .84 .35 sign sought\n",
    "\n",
    ".93 stop ...\n",
    "_____\n",
    "\n",
    "This representation is not hard to evolve because the only hard guesses are the vectorSpace purchase, the clusterer, and the construct while the rest have a smooth gradient. A rich ecosystem of problems on the blackboard that include other vector spaces besides the one ordered here would help to make this agent even easier to evolve, because these may be more. Because the cluster takes in two inputs, at most only two more construct or buy blocks are allowed, and then there is an automatic stop. In this case the stop evolved anyway, so that the bought item is used twice.\n",
    "\n",
    "\n",
    "Transactions only go through if all pieces are present, but assuming that will happen, this is the translation of the code that the human has purchased so far:\n",
    "\n",
    "_____\n",
    "\n",
    "blackboard['test_clusterer_silouhette']\n",
    "\n",
    "(blackboard['clusterer_sklearn_kmeans_20clusters']\n",
    "\n",
    "(vectorSpace\n",
    "\n",
    ")) \n",
    "\n",
    "(vectorSpace)\n",
    "\n",
    "_____\n",
    "\n",
    "Each entry in the blackboard dictionary is the curry corresponding to its name\n",
    "Since there is a fork, another agent is needed to create the vector space. \n",
    "We use only one to demonstrate a minimal agent setup.  It is harder to evolve because it involves the data guess, the vector guess, and multiple construction guesses. However, the multiple construction of preprocessors have gradient because the program will still work and get a gradient answer from test.  The use of the sign field by communicating agents would make it easier still to evolve, as will be demonstrated in our agent based coevolution section.  \n",
    "\n",
    "_____\n",
    "\n",
    "\n",
    ".63 .52 .54 .82 .91 .05 .22 .57 sign displayed\n",
    "\n",
    "0.08 construct\n",
    "\n",
    "0.73 vectorSpace  0.84 doc2vec 0.11 gensim .88 size200 .79 iterations1000  .62 minfreq5  .99 stop \n",
    "\n",
    ".28 ... test type not chosen \n",
    "\n",
    "0.34 0.41 accepts between 34 and 41 agiTokens \n",
    "\n",
    ".53 .25 .34 .46 .76 .22 .81 .75 sign sought\n",
    "\n",
    "0.18 construct\n",
    "\n",
    "0.40 preprocessor  0.23 freetext 0.13 emojiRemoval .20 ...\n",
    "\n",
    ".28 ...test type not chosen \n",
    "\n",
    ".90 stop...\n",
    "\n",
    ".93 stop...\n",
    "\n",
    "0.13 construct\n",
    "\n",
    "0.43 preprocessor  0.46 freetext 0.13 lemmatization .51 ...\n",
    "\n",
    ".28 ...test type not chosen \n",
    "\n",
    ".96 stop...\n",
    "\n",
    ".96 stop...\n",
    "\n",
    "0.03 construct\n",
    "\n",
    "0.39 preprocessor  0.88 freetext 0.13 stopwords .82 ...\n",
    "\n",
    ".28 ...test type not chosen \n",
    "\n",
    ".98 stop...\n",
    "\n",
    ".97 stop...\n",
    "\n",
    "_____\n",
    "\n",
    "\n",
    "\n",
    "This translates to a complete program:\n",
    "\n",
    "_____\n",
    "  \n",
    "\n",
    "ontology['test_clusterer_silouhette']\n",
    "\n",
    "(ontology['clusterer_sklearn_kmeans_20clusters']\n",
    "\n",
    "(vectorSpace\n",
    "\n",
    ")) \n",
    "\n",
    "(vectorSpace)\n",
    "\n",
    "vectorSpace=ontology['vectorSpace_gensim_doc2vec_size200_iterations1000_minfreq5']\n",
    "\n",
    "(data = ontology['preprocessor_freetext_emoji_removal']\n",
    "\n",
    "(data = ontology['preprocessor_freetext_lemmatization']\n",
    "\n",
    "(data = ontology['preprocessor_freetext_stopword']\n",
    "\n",
    "(data = ontology['data_freetext_internetResearchAgency']\n",
    "\n",
    "))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Python Program from an evolvable representation through Currying\n",
    "\n",
    "First we will demonstrate the python curry function, from https://mtomassoli.wordpress.com/2012/03/18/currying-in-python/ by Massimiliano Tomassoli, 2012, applied to normal nlp python programs, mentioned in the above example of an ontology and blackboard communication. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Massimiliano Tomassoli, 2012.\n",
    "#\n",
    "\n",
    "\n",
    "def genCur(func, unique = True, minArgs = None):\n",
    "    \"\"\" Generates a 'curried' version of a function. \"\"\"\n",
    "    def g(*myArgs, **myKwArgs):\n",
    "        def f(*args, **kwArgs):\n",
    "            if args or kwArgs:                  # some more args!\n",
    "                # Allocates data to assign to the next 'f'.\n",
    "                newArgs = myArgs + args\n",
    "                newKwArgs = dict.copy(myKwArgs)\n",
    "\n",
    "                # If unique is True, we don't want repeated keyword arguments.\n",
    "                if unique and not kwArgs.keys().isdisjoint(newKwArgs):\n",
    "                    raise ValueError(\"Repeated kw arg while unique = True\")\n",
    "\n",
    "                # Adds/updates keyword arguments.\n",
    "                newKwArgs.update(kwArgs)\n",
    "\n",
    "                # Checks whether it's time to evaluate func.\n",
    "                if minArgs is not None and minArgs <= len(newArgs) + len(newKwArgs):\n",
    "                    return func(*newArgs, **newKwArgs)  # time to evaluate func\n",
    "                else:\n",
    "                    return g(*newArgs, **newKwArgs)     # returns a new 'f'\n",
    "            else:                               # the evaluation was forced\n",
    "                return func(*myArgs, **myKwArgs)\n",
    "        return f\n",
    "    return g\n",
    "\n",
    "def cur(f, minArgs = None):\n",
    "    return genCur(f, True, minArgs)\n",
    "\n",
    "def curr(f, minArgs = None):\n",
    "    return genCur(f, False, minArgs)\n",
    "\n",
    "# Simple Function.\n",
    "def func(a, b, c, d, e, f, g = 100):\n",
    "    print(a, b, c, d, e, f, g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clusterer_silhouette(X,Y):\n",
    "    \n",
    "    # \"_args\": [{  \"type\": \"numpy.ndarray\",  \"dtype\": \"float32\"},\n",
    "    #           {\"type\": \"numpy.ndarray\", \"dtype\": \"int32\"}],\n",
    "    # \"_return\": [{\"type\": \"float\"}]\n",
    "    \n",
    "    # we only want to test cosine metric for this example, but it could be a parameter in other cases\n",
    "    \n",
    "    import sklearn\n",
    "    from sklearn import metrics\n",
    "    print('test_clusterer_silhouette')\n",
    "    silhouette = metrics.silhouette_score(X, Y, metric = 'cosine')\n",
    "    return (silhouette)\n",
    "\n",
    "def test_clusterer_calinskiHarabaz(X,Y):\n",
    "    \n",
    "    # \"_args\": [{  \"type\": \"numpy.ndarray\",  \"dtype\": \"float32\"},\n",
    "    #           {\"type\": \"numpy.ndarray\", \"dtype\": \"int32\"}],\n",
    "    # \"_return\": [{\"type\": \"float\"}]\n",
    "    \n",
    "    # we only want to test cosine metric for this example, but it could be a parameter in other cases\n",
    "    \n",
    "    import sklearn\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    calinski_harabaz = metrics.calinski_harabaz_score(X, clusterAlgLabelAssignmentsSD) \n",
    "    return (calinski_harabaz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the NLP routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorSpace_gensim_doc2vec (X,size,iterations, minfreq):\n",
    "    \n",
    "    \n",
    "     #   \"_args\": [{\"type\": \"list\",\"firstElement\":\"gensim.models.doc2vec.TaggedDocument\" }],\n",
    "     #   \"_return\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\" }\n",
    "    \n",
    "    import gensim\n",
    "    import numpy as np\n",
    "    import sklearn.preprocessing\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    \n",
    "    print('vectorSpace_gensim_doc2vec')\n",
    "    \n",
    "    \n",
    "    model = gensim.models.doc2vec.Doc2Vec(size=size, min_count=minfreq,iter = iterations,dm=0)\n",
    "    model.build_vocab(X)\n",
    "    model.train(X, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    cmtVectors = [model.infer_vector(X[i].words) for i in range(len(X)) ]\n",
    "    cmtVectors = [inferred_vector for inferred_vector in cmtVectors \n",
    "                  if  not np.isnan(inferred_vector).any() \n",
    "                  and not np.isinf(inferred_vector).any()]\n",
    "   \n",
    "    X = StandardScaler().fit_transform(cmtVectors)\n",
    "    return(X)\n",
    "    \n",
    "def preprocessor_freetext_tag (X):\n",
    "    \n",
    "    #convert a list of strings to a tagged document\n",
    "    #if it is a list of a list of strings broadcast to a list of tagged documents\n",
    "\n",
    "\n",
    "    #   \"_args\": [{\"type\": \"list\",\"firstElement\":\"string\" }],\n",
    "    #   \"_return\": [{\"type\": \"list\",\"gensim.models.doc2vec.TaggedDocument\" }]\n",
    "    \n",
    "    import gensim\n",
    "    print ('preprocessor_freetext_tag')\n",
    "    \n",
    "    tag = lambda x,y: gensim.models.doc2vec.TaggedDocument(x,[y])\n",
    "    \n",
    "    if type(X) is str:\n",
    "        tagged = tag(X,X)\n",
    "    else:\n",
    "        tagged = [tag(x,y) for y,x in enumerate(X)]\n",
    "    return (tagged)\n",
    "    \n",
    "def preprocessor_freetext_lemmatization (X):\n",
    "    \n",
    "    #   \"_args\": [{\"type\": \"list\",\"firstElement\":\"string\" }],\n",
    "    #   \"_return\": [{\"type\": \"list\",\"firstElement\":\"list\" }]\n",
    "    \n",
    "    #converts string documents into list of tokens\n",
    "    #if given a list, broadcasts\n",
    "    import gensim\n",
    "    \n",
    "    print('preprocessor_freetext_lemmatization')\n",
    "    stopfile = 'stopwords.txt'\n",
    "    lemmatized = []\n",
    "    with open(stopfile,'r') as f:\n",
    "        stopwords = {word.lower().strip() for word in f.readlines()}\n",
    "        lemma = lambda x:[b.decode('utf-8') for b in gensim.utils.lemmatize(str(x),stopwords=frozenset(stopwords)) ]\n",
    "    \n",
    "        if type(X) is str:\n",
    "            lemmatized = lemma(X)\n",
    "        else:\n",
    "            lemmatized = [lemma(x) for x in X]\n",
    "    \n",
    "    return(lemmatized)\n",
    "    \n",
    "\n",
    "    \n",
    "def preprocessor_freetext_strip (X):\n",
    "    \n",
    "    # strips addresses and emojis. if you get string strip, if you get list broadcast\n",
    "    \n",
    "    #   \"_args\": [{\"type\": \"list\",\"firstElement\":\"string\" }],\n",
    "    #   \"_return\": [{\"type\": \"list\",\"firstElement\":\"string\" }]\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    print(\"preprocessor_freetext_strip\")\n",
    "    code ='utf-8'\n",
    "    strip = lambda  x: re.sub(r\"\\s?http\\S*\", \"\", x).encode(code).decode(code)\n",
    "    \n",
    "    #strip = lambda  x: re.sub(r\"\\s?http\\S*\", \"\", x).decode(code)\n",
    "    #strip = lambda  x: re.sub(r\"\\s?http\\S*\", \"\", x.decode(code))\n",
    "    #strip = lambda  x: re.sub(r\"\\s?http\\S*\", \"\", x)\n",
    "    \n",
    "    if type(X) is str:\n",
    "        decoded = strip(X)\n",
    "    else:\n",
    "        decoded = [strip(x) for x in X]\n",
    "    return (decoded)\n",
    "\n",
    "       \n",
    "def preprocessor_freetext_shuffle (X):\n",
    "    \n",
    "    #   \"_args\": [{\"type\": \"list\" }],\n",
    "    #   \"_return\": [{\"type\": \"list\" }]\n",
    "    import random\n",
    "    print(\"preprocessor_freetext_shuffle\")\n",
    "    random.shuffle(X)\n",
    "    return (X)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_freetext_csvColumn(path, col = 'text'):\n",
    "    #  returns a list of documents that are strings\n",
    "    #   \"_return\": [{\"type\": \"list\",\"firstElement\":\"string\" }]\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    print('data_freetext_csvColumn_short')\n",
    "    raw_data = pd.read_csv(path, encoding = \"ISO-8859-1\")\n",
    "    docList = [raw_data.loc[i,col] for i in range (len(raw_data)) if raw_data.loc[i,col]]\n",
    "    return docList\n",
    "\n",
    "def data_vector_blobs(n_samples = 1500):\n",
    "    import sklearn\n",
    "    from sklearn.datasets import make_blobs\n",
    "    X,Y = make_blobs(n_samples=n_samples, random_state=8)\n",
    "    return X\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the clusterers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterer_sklearn_kmeans(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import MiniBatchKMeans\n",
    "    \n",
    "    print ('clusterer_sklearn_kmeans')\n",
    "    clusterAlgSKN = MiniBatchKMeans(n_clusters).fit(X)\n",
    "    clusterAlgLabelAssignmentsSKN= clusterAlgSKN.predict(X)\n",
    "    return (clusterAlgLabelAssignmentsSKN)\n",
    "\n",
    "\n",
    "def clusterer_sklearn_agglomerative(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    \n",
    "    average_linkage = AgglomerativeClustering(linkage=\"average\", \n",
    "        affinity=\"cosine\",n_clusters=params['n_clusters'], connectivity=connectivity).fit(X)\n",
    "    clusterAlgLabelAssignmentsSAG= average_linkage.labels_.astype(np.int)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsSAG)\n",
    "\n",
    "def clusterer_sklearn_affinityPropagation(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import AffinityPropagation\n",
    "    \n",
    "    affinity_propagation = cluster.AffinityPropagation(damping=params['damping'], preference=params['preference']).fit(X)\n",
    "    clusterAlgLabelAssignmentsSAP= affinity_propagation.predict(X)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsSAP)\n",
    "\n",
    "\n",
    "def clusterer_sklearn_meanShift(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import MeanShift\n",
    "    \n",
    "    \n",
    "    bandwidth = sklearn.cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "    \n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True).fit(X)\n",
    "    clusterAlgLabelAssignmentsSM= ms.predict(X)\n",
    "        \n",
    "    return (clusterAlgLabelAssignmentsSM)\n",
    "\n",
    "def clusterer_sklearn_spectral(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    \n",
    "    spectral = SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"cosine\")\n",
    "    try:\n",
    "        clusterAlgLabelAssignmentsSS= None\n",
    "        spectral = spectral.fit(X)\n",
    "    except ValueError as e:\n",
    "        pass\n",
    "    else:\n",
    "        clusterAlgLabelAssignmentsSS= spectral.labels_.astype(np.int)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsSS)\n",
    "\n",
    "\n",
    "def clusterer_sklearn_ward(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "    ward = AgglomerativeClustering(n_clusters=params['n_clusters'], linkage='ward',\n",
    "                                   connectivity=connectivity).fit(X)\n",
    "    clusterAlgLabelAssignmentsSW= ward.labels_.astype(np.int)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsSW)\n",
    "\n",
    "\n",
    "def clusterer_sklearn_dbscan(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    \n",
    "    dbscan = DBSCAN(eps=params['eps']).fit(X)\n",
    "    clusterAlgLabelAssignmentsSD= dbscan.labels_.astype(np.int)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsSD)\n",
    "\n",
    "\n",
    "def clusterer_sklearn_birch(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import Birch\n",
    "    \n",
    "    \n",
    "    birch = Birch(n_clusters=params['n_clusters']).fit(X)\n",
    "    clusterAlgLabelAssignmentsSB= birch.predict(X)\n",
    "        \n",
    "    return (clusterAlgLabelAssignmentsSB)\n",
    "\n",
    "\n",
    "def clusterer_sklearn_gaussian(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn import mixture\n",
    "    \n",
    "    clusterAlgSGN = mixture.GaussianMixture(n_components=params['n_clusters'], covariance_type='full').fit(X)\n",
    "    clusterAlgLabelAssignmentsSGN= clusterAlgSGN.predict(X)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsSGN)\n",
    "\n",
    "\n",
    "def clusterer_nltk_kmeans(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import nltk\n",
    "    from nltk.cluster.kmeans import KMeansClusterer\n",
    "    \n",
    "    \n",
    "    clusterAlgNK = KMeansClusterer(params['n_clusters'], distance=nltk.cluster.util.cosine_distance, repeats=25, avoid_empty_clusters=True)\n",
    "    clusterAlgLabelAssignmentsNK = clusterAlgNK.cluster(cmtVectors, assign_clusters=True)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsNK)\n",
    "\n",
    "\n",
    "def clusterer_nltk_agglomerative(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import nltk\n",
    "    from nltk.cluster.gaac import GAAClusterer\n",
    "    \n",
    "    \n",
    "    clusterAlgNG = GAAClusterer(num_clusters=params['n_clusters'], normalise=True, svd_dimensions=None)\n",
    "    clusterAlgLabelAssignmentsNG = clusterAlgNG.cluster(cmtVectors, assign_clusters=True)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsNG)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill the initial function\n",
    "ontology= {}\n",
    "ontology['data_freetext_csvColumn']= curr(data_freetext_csvColumn)\n",
    "ontology['data_vector_blobs']= curr(data_vector_blobs)\n",
    "ontology['preprocessor_freetext_shuffle'] = curr(preprocessor_freetext_shuffle)\n",
    "ontology['preprocessor_freetext_strip'] = curr(preprocessor_freetext_strip)\n",
    "ontology['preprocessor_freetext_lemmatization']  = curr(preprocessor_freetext_lemmatization)\n",
    "ontology['preprocessor_freetext_tag']= curr(preprocessor_freetext_tag)\n",
    "ontology['vectorSpace_gensim_doc2vec'] = curr(vectorSpace_gensim_doc2vec)\n",
    "ontology['clusterer_sklearn_kmeans'] = curr (clusterer_sklearn_kmeans)\n",
    "ontology['clusterer_sklearn_agglomerative'] = curr (clusterer_sklearn_agglomerative)\n",
    "ontology['clusterer_sklearn_affinityPropagation'] = curr (clusterer_sklearn_affinityPropagation)\n",
    "ontology['clusterer_sklearn_meanShift'] = curr (clusterer_sklearn_meanShift)\n",
    "ontology['clusterer_sklearn_spectral'] = curr (clusterer_sklearn_spectral)\n",
    "ontology['clusterer_sklearn_ward'] = curr (clusterer_sklearn_ward)\n",
    "ontology['clusterer_sklearn_dbscan'] = curr (clusterer_sklearn_dbscan)\n",
    "ontology['clusterer_sklearn_birch'] = curr (clusterer_sklearn_birch)\n",
    "ontology['clusterer_sklearn_gaussian'] = curr (clusterer_sklearn_gaussian)\n",
    "ontology['clusterer_nltk_agglomerative'] = curr (clusterer_nltk_agglomerative)\n",
    "ontology['clusterer_nltk_kmeans'] = curr (clusterer_nltk_kmeans)\n",
    "ontology['test_clusterer_silhouette']  = curr(test_clusterer_silhouette)\n",
    "ontology['test_clusterer_calinskiHarabaz']  = curr(test_clusterer_calinskiHarabaz)\n",
    "\n",
    "#Create the constructions that would be machine learned, using a shortened dataset\n",
    "\n",
    "\n",
    "ontology['data_freetext_csvColumn_short']= ontology['data_freetext_csvColumn'](path = 'short.csv')\n",
    "ontology['clusterer_sklearn_kmeans_20clusters'] = ontology['clusterer_sklearn_kmeans'](n_clusters = 20)\n",
    "ontology['vectorSpace_gensim_doc2vec_size200_iterations1000_minfreq5']= ontology['vectorSpace_gensim_doc2vec'](size=200)(iterations = 1000)(minfreq = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_freetext_csvColumn_short\n",
      "preprocessor_freetext_shuffle\n",
      "preprocessor_freetext_strip\n",
      "preprocessor_freetext_lemmatization\n",
      "preprocessor_freetext_tag\n",
      "vectorSpace_gensim_doc2vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/debbie/anaconda3/envs/simulation/lib/python3.6/site-packages/gensim/models/doc2vec.py:362: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/home/debbie/anaconda3/envs/simulation/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "/home/debbie/anaconda3/envs/simulation/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusterer_sklearn_kmeans\n",
      "test_clusterer_silhouette\n"
     ]
    }
   ],
   "source": [
    "# First agent\n",
    "a = ontology['data_freetext_csvColumn_short']()\n",
    "b = ontology['preprocessor_freetext_shuffle'](a)()\n",
    "c = ontology['preprocessor_freetext_strip'](b)()\n",
    "d = ontology['preprocessor_freetext_lemmatization'](c)()\n",
    "e = ontology['preprocessor_freetext_tag'](d)()\n",
    "f = ontology['vectorSpace_gensim_doc2vec_size200_iterations1000_minfreq5'](e)()\n",
    "\n",
    "# Second agent\n",
    "g = ontology['clusterer_sklearn_kmeans_20clusters'](f)()\n",
    "h = ontology['test_clusterer_silhouette'] (f)(g)()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all the parts of the NLP solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @LettyNTX: Donald Trump Is Winning on Facebook  http://t.co/Chabu4oRt2 #Newsmax via @Newsmax_Media',\n",
       " 'RT @cutupx2: \\x9f\\x94\\x9f\\x94Complacency will not give you anymore than what you already have. Change is to take the Risk, without Risk nothing new is ev\\x80',\n",
       " 'RT @KeithOlbermann: Can we go for PRE-impeachment of Boss @realDonaldTrump Tweed? https://t.co/RKF0kroJH6',\n",
       " 'RT @PrisonPlanet: Police told to stand down yet again. Sessions needs to call a federal investigation. https://t.co/aakfcuaZhi',\n",
       " 'RT @HomerWhite: So. My Prius could too :) https://t.co/JrsXRrnqy9']"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @LettyNTX: Donald Trump Is Winning on Facebook  http://t.co/Chabu4oRt2 #Newsmax via @Newsmax_Media',\n",
       " 'RT @cutupx2: \\x9f\\x94\\x9f\\x94Complacency will not give you anymore than what you already have. Change is to take the Risk, without Risk nothing new is ev\\x80',\n",
       " 'RT @KeithOlbermann: Can we go for PRE-impeachment of Boss @realDonaldTrump Tweed? https://t.co/RKF0kroJH6',\n",
       " 'RT @PrisonPlanet: Police told to stand down yet again. Sessions needs to call a federal investigation. https://t.co/aakfcuaZhi',\n",
       " 'RT @HomerWhite: So. My Prius could too :) https://t.co/JrsXRrnqy9']"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @LettyNTX: Donald Trump Is Winning on Facebook  #Newsmax via @Newsmax_Media',\n",
       " 'RT @cutupx2: \\x9f\\x94\\x9f\\x94Complacency will not give you anymore than what you already have. Change is to take the Risk, without Risk nothing new is ev\\x80',\n",
       " 'RT @KeithOlbermann: Can we go for PRE-impeachment of Boss @realDonaldTrump Tweed?',\n",
       " 'RT @PrisonPlanet: Police told to stand down yet again. Sessions needs to call a federal investigation.',\n",
       " 'RT @HomerWhite: So. My Prius could too :)']"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rt/NN',\n",
       "  'lettyntx/NN',\n",
       "  'donald/JJ',\n",
       "  'trump/NN',\n",
       "  'win/VB',\n",
       "  'facebook/NN',\n",
       "  'newsmax/NN',\n",
       "  'newsmax_media/NN'],\n",
       " ['rt/NN',\n",
       "  'cutupx/NN',\n",
       "  'complacency/NN',\n",
       "  'give/VB',\n",
       "  'anymore/RB',\n",
       "  'already/RB',\n",
       "  'change/NN',\n",
       "  'take/VB',\n",
       "  'risk/NN',\n",
       "  'risk/NN',\n",
       "  'nothing/NN',\n",
       "  'new/JJ',\n",
       "  'ev/JJ'],\n",
       " ['rt/NN',\n",
       "  'keitholbermann/NN',\n",
       "  'go/VB',\n",
       "  'pre/NN',\n",
       "  'impeachment/NN',\n",
       "  'boss/NN',\n",
       "  'realdonaldtrump/JJ',\n",
       "  'tweed/NN'],\n",
       " ['rt/NN',\n",
       "  'prisonplanet/NN',\n",
       "  'polouse/NN',\n",
       "  'tell/VB',\n",
       "  'stand/VB',\n",
       "  'yet/RB',\n",
       "  'session/NN',\n",
       "  'need/NN',\n",
       "  'call/VB',\n",
       "  'federal/JJ',\n",
       "  'investigation/NN'],\n",
       " ['rt/NN', 'homerwhite/NN', 'prius/NN']]"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['rt/NN', 'lettyntx/NN', 'donald/JJ', 'trump/NN', 'win/VB', 'facebook/NN', 'newsmax/NN', 'newsmax_media/NN'], tags=[0]),\n",
       " TaggedDocument(words=['rt/NN', 'cutupx/NN', 'complacency/NN', 'give/VB', 'anymore/RB', 'already/RB', 'change/NN', 'take/VB', 'risk/NN', 'risk/NN', 'nothing/NN', 'new/JJ', 'ev/JJ'], tags=[1]),\n",
       " TaggedDocument(words=['rt/NN', 'keitholbermann/NN', 'go/VB', 'pre/NN', 'impeachment/NN', 'boss/NN', 'realdonaldtrump/JJ', 'tweed/NN'], tags=[2]),\n",
       " TaggedDocument(words=['rt/NN', 'prisonplanet/NN', 'polouse/NN', 'tell/VB', 'stand/VB', 'yet/RB', 'session/NN', 'need/NN', 'call/VB', 'federal/JJ', 'investigation/NN'], tags=[3]),\n",
       " TaggedDocument(words=['rt/NN', 'homerwhite/NN', 'prius/NN'], tags=[4])]"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.609496  , -0.3483922 , -1.86715073,  0.36264013,  0.21943951,\n",
       "        -0.03532237, -0.0532568 , -0.48242199, -1.05669617, -1.10821146,\n",
       "        -0.26455432,  0.66196379, -0.14440665,  0.36850437, -0.93290371,\n",
       "        -0.69900582, -0.73845936,  0.52014146, -0.83977007,  0.07206511,\n",
       "        -0.0157043 , -0.27976841,  0.40680538, -1.39225733,  0.99830578,\n",
       "        -0.39829933,  0.69819823, -0.30565271,  0.10508791, -0.07052414,\n",
       "        -0.77301518,  0.20885887, -0.25100282, -0.37686484,  0.84764341,\n",
       "         0.57364489,  0.68937192, -0.64805723, -0.09089956, -0.9137106 ,\n",
       "        -1.30326029,  0.26541536,  0.62573907, -1.38591966,  0.06241144,\n",
       "        -1.31003105, -2.0438508 ,  0.67112336, -0.81232505, -1.02079578,\n",
       "         0.74932547, -0.8874089 ,  0.58430748,  0.39959196, -0.01114771,\n",
       "         0.66178072, -0.28492291, -0.89978613,  1.03584804,  0.42428559,\n",
       "        -0.15191966, -0.45002769, -0.03826814, -0.23649364,  0.51134102,\n",
       "         0.08314528, -0.76107961,  1.52860465,  0.10839723, -0.59170164,\n",
       "        -1.3941027 , -0.11364899,  2.06633062,  1.11602833,  0.05229145,\n",
       "        -1.12941324, -0.24954926,  0.11924809, -1.01575024,  0.32322448,\n",
       "        -1.28486959,  0.53846836, -0.72277372,  0.81415625, -0.47001769,\n",
       "        -0.35603639,  0.11825469,  0.36428373,  0.35956287, -0.08664654,\n",
       "        -1.28483372, -0.34646681,  0.03715635,  0.2053088 , -1.01150343,\n",
       "         0.12369093, -0.06093221, -0.47314234, -0.26522004,  0.29569434,\n",
       "        -0.80307855, -0.03597539, -0.6982081 , -0.73157395,  1.1166487 ,\n",
       "         0.46364443,  0.79916968,  1.05885501,  1.1688495 ,  0.14672522,\n",
       "         0.29756711, -0.57269804, -0.06664337, -1.69719926, -0.9295866 ,\n",
       "        -0.21452462,  0.48542492,  0.16936587,  0.22200919,  0.38835588,\n",
       "         0.62224832, -1.86151734,  0.02158907,  0.58662945,  0.71837219,\n",
       "        -0.58453806, -0.09604837,  0.45971632,  0.07590868, -0.21681005,\n",
       "         1.36702222,  0.66975973,  0.79347362,  0.64211772, -1.00863833,\n",
       "        -0.27844932, -1.07549847,  1.39106283,  0.15154329, -0.57516432,\n",
       "         1.35938447,  0.2002984 ,  1.21542399,  0.9751465 , -0.18515411,\n",
       "        -0.66617861, -0.35628622, -0.54311887,  0.65139438,  0.54793364,\n",
       "        -0.27806996, -2.14099348,  0.0642387 , -0.35995112, -0.64467168,\n",
       "        -0.45059272,  0.1084087 , -0.49251244,  0.72938851, -0.44383393,\n",
       "        -0.19274458, -0.81548014,  0.6330764 ,  0.90325784, -0.84102516,\n",
       "         0.10443036,  0.37258388,  1.12881927,  0.31818258, -0.03054939,\n",
       "         0.19179286, -1.19454718, -0.15052112, -1.03547548, -0.20815372,\n",
       "         0.94307062,  1.27905125,  0.00693185,  0.00690094,  0.55247264,\n",
       "        -2.16849996, -0.27322584, -1.28136261,  0.611735  ,  2.01262124,\n",
       "         0.34212415,  0.06296853, -0.4674222 , -0.5368777 , -0.36223795,\n",
       "        -0.12182084,  0.6247249 , -0.9075119 ,  0.52151306,  0.15365081,\n",
       "        -0.47405551, -0.20134757, -0.27711869, -0.23506823, -0.04527663]])"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  1, 11,  4,  0], dtype=int32)"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13341730121191805"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part we have examined a general, flexible, and evolvable representation for the agent communication and for the ontology.  We have generated python program from agent communications about buying, selling, and constructing software.  This representation can make use of statistics about the frequency of functions occurring in problem types, such as in Microsoft's Deep coder.  In part two we will address a coevolutionary representation that leverages a rich heterogeneous environment to make the evolution of python programs within reach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
